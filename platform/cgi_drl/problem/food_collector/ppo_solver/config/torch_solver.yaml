baseline:
    environment: [cgi_drl.environment.unity_gym.config.environment_template, DefaultTemplate, environment/unity_gym/config/environment.yaml, food_collector]
    observation_preprocessor: [cgi_drl.environment.unity_gym.config.observation_preprocessor_template, FoodCollectorTemplate, environment/unity_gym/config/observation_preprocessor.yaml, default]
    ppo: [cgi_drl.decision_model.ppo.config.food_collector.model_template_torch, DefaultTemplate, decision_model/ppo/config/food_collector/model_torch.yaml, default]
    gae: [cgi_drl.data_storage.gae_sample_memory.config.unity_gym_template, DefaultTemplate, data_storage/gae_sample_memory/config/unity_gym.yaml, default]
    
    version: versions/food_collector_ppo_baseline
    batch_size: 128
    is_load_policy: false
    training_steps: 10000000
    epoch_steps: 100000
    update_sample_count: 5120
    update_epoch_count: 3
    max_game_step: 5000 
    discount_factor_gamma: 0.99
    discount_factor_lambda: 0.95

    evaluation_max_game_step: 5000
    evaluation_episode_count: 8

constant_cost:
    environment: [cgi_drl.environment.unity_gym.config.environment_template, DefaultTemplate, environment/unity_gym/config/environment.yaml, food_collector]
    observation_preprocessor: [cgi_drl.environment.unity_gym.config.observation_preprocessor_template, FoodCollectorTemplate, environment/unity_gym/config/observation_preprocessor.yaml, default]
    ppo: [cgi_drl.decision_model.ppo.config.food_collector.model_template_torch, DefaultTemplate, decision_model/ppo/config/food_collector/model_torch.yaml, default]
    gae: [cgi_drl.data_storage.gae_sample_memory.config.unity_gym_template, DefaultTemplate, data_storage/gae_sample_memory/config/unity_gym.yaml, default]
    
    version: versions/food_collector_ppo_constant_cost
    batch_size: 128
    is_load_policy: false
    training_steps: 10000000
    epoch_steps: 100000
    update_sample_count: 5120
    update_epoch_count: 3
    max_game_step: 5000 
    discount_factor_gamma: 0.99
    discount_factor_lambda: 0.95

    evaluation_max_game_step: 5000
    evaluation_episode_count: 8

abc_rl_ppo:
    environment: [cgi_drl.environment.unity_gym.config.environment_template, DefaultTemplate, environment/unity_gym/config/environment.yaml, food_collector]
    observation_preprocessor: [cgi_drl.environment.unity_gym.config.observation_preprocessor_template, FoodCollectorTemplate, environment/unity_gym/config/observation_preprocessor.yaml, default]
    ppo: [cgi_drl.decision_model.ppo.config.food_collector.model_template_torch, DefaultTemplate, decision_model/ppo/config/food_collector/model_torch.yaml, default]
    gae: [cgi_drl.data_storage.gae_sample_memory.config.unity_gym_template, DefaultTemplate, data_storage/gae_sample_memory/config/unity_gym.yaml, default]
    
    version: versions/food_collector_abc_rl_ppo
    batch_size: 128
    is_load_policy: false
    training_steps: 10000000
    epoch_steps: 100000
    update_sample_count: 5120
    update_epoch_count: 3
    max_game_step: 5000 
    discount_factor_gamma: 0.99
    discount_factor_lambda: 0.95

    evaluation_max_game_step: 5000
    evaluation_episode_count: 8
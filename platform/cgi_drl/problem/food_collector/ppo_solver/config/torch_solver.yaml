baseline:
    environment: [cgi_drl.default_config, Default, environment/unity_gym/config/environment.yaml, food_collector]
    ppo: [cgi_drl.decision_model.ppo.config.food_collector.model_template, DefaultTemplate, decision_model/ppo/config/atari/model.yaml, default]
    gae: [cgi_drl.data_storage.gae_sample_memory.config.unity_gym_template, DefaultTemplate, data_storage/gae_sample_memory/config/unity_gym.yaml, default]
    
    version: versions/food_collector_ppo_baseline
    batch_size: 128
    is_load_policy: false
    training_steps: 10000000
    epoch_steps: 100000
    update_sample_count: 5120
    update_epoch_count: 3
    max_game_step: 5000 
    discount_factor_gamma: 0.99
    discount_factor_lambda: 0.95

    evaluation_max_game_step: 5000
    evaluation_episode_count: 5

constant_cost:
    environment: [cgi_drl.default_config, Default, environment/unity_gym/config/environment.yaml, food_collector]
    ppo: [cgi_drl.decision_model.ppo.config.food_collector.model_template, DefaultTemplate, decision_model/ppo/config/atari/model.yaml, default]
    gae: [cgi_drl.data_storage.gae_sample_memory.config.unity_gym_template, DefaultTemplate, data_storage/gae_sample_memory/config/unity_gym.yaml, default]
    
    version: versions/food_collector_ppo_constant_cost
    batch_size: 128
    is_load_policy: false
    training_steps: 10000000
    epoch_steps: 100000
    update_sample_count: 5120
    update_epoch_count: 3
    max_game_step: 5000 
    discount_factor_gamma: 0.99
    discount_factor_lambda: 0.95

    evaluation_max_game_step: 5000
    evaluation_episode_count: 5

abc_rl_ppo:
    environment: [cgi_drl.default_config, Default, environment/unity_gym/config/environment.yaml, food_collector]
    ppo: [cgi_drl.decision_model.ppo.config.food_collector.model_template, DefaultTemplate, decision_model/ppo/config/atari/model.yaml, default]
    gae: [cgi_drl.data_storage.gae_sample_memory.config.unity_gym_template, DefaultTemplate, data_storage/gae_sample_memory/config/unity_gym.yaml, default]
    
    version: versions/food_collector_abc_rl_ppo
    batch_size: 128
    is_load_policy: false
    training_steps: 10000000
    epoch_steps: 100000
    update_sample_count: 5120
    update_epoch_count: 3
    max_game_step: 5000 
    discount_factor_gamma: 0.99
    discount_factor_lambda: 0.95

    score_threshold_Vth: 32
    historical_score_window_size_h: 10

    evaluation_max_game_step: 5000
    evaluation_episode_count: 5